{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.contrib.rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BasicRNNCell : The most basic RNN cell\n",
    "* RNNCell : Abstract object representing an RNN cell\n",
    "* BasicLSTMCell : Basic LSTM recurrent network cell\n",
    "* LSTMCell : LSTM recurrent network cell\n",
    "* GRUCell : Gated Recurrent Unit cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_size =50\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell = tf.contrib.rnn.GRUCell(hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack multiple cells "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.dynamic_rnn : uses a tf.While loop to dynamically construct the graph when it is executed. Graph creation is faster and you can feed batched of variable size\n",
    "* tf.nn.bidirectional_dynamic_rnn : dynamic_rnn with bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn_cells = tf.contrib.rnn.MultiRNNCell([cell]*num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, <strong>length</strong>, initial_state) <br>\n",
    "<br>\n",
    "대부분의 시퀀스들은 길이가 다르다. 어떡할까"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with variable sequence length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "길이가 같아지도록 zero vecotor로 padding 해준다. <br>\n",
    "대부분의 현존하는 모델들은 120 토큰보다 긴 시퀀스에는 잘 대처하지 못한다. 그래서 보통은 max_length를 고정해 놓고, 그것보다 길면 쳐낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padded/truncated sequence length "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 0으로 padded된 레이블은 total loss에 영향을 미치고 gradients에 영향을 주게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maintain a mast(True for real, False for padded tokens) 진짜와 padding된 토큰을 구분해둔다\n",
    "* Run your model on both the real/padded tokens(model will predict labels for the padded tokens as well)\n",
    "* Only take into account the loss caused by the real elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels) <br>\n",
    "loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제 loss 계산할 때, 진짜 토큰만 고려한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let your model know the real sequence length so it only predict the labels for the real tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size) <br>\n",
    "rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers) <br>\n",
    "<strong>length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)</strong> <br>\n",
    "output, out_state = tf.nn.dynamic_rnn(cell, seq, <strong>length</strong>, initial_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use different activation units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.relu\n",
    "* tf.nn.relu6\n",
    "* tf.nn.crelu\n",
    "* tf.nn.elu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In addition to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tf.nn.softplus\n",
    "* tf.nn.softsign\n",
    "* tf.nn.bias_add\n",
    "* tf.sigmoid\n",
    "* tf.tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploding Gradients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip gradients with tf.clip_by_global_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gradients = tf.gradients(cost, tf.trainable_variables()) <br>\n",
    "clipped\\_gradients, _ = tf.clip_by_global_norm(gradients, max_grad_norm) <br>\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) <br>\n",
    "train_op = optimizer.apply_gradients(zip(gradients, trainables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructs symbolic partial derivatives of sum of ys w.r.t. x in xs.\n",
    "\n",
    "ys and xs are each a Tensor or a list of tensors. grad_ys is a list of Tensor, holding the gradients received by the ys. The list must be the same length as ys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(symbolic) Jacobian Matrix를 돌려주는건가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clips values of multiple tensors by the ratio of the sum of their norms.\n",
    "\n",
    "To perform the clipping, the values t_list[i] are set to:\n",
    "\n",
    "t_list[i] * clip_norm / max(global_norm, clip_norm)\n",
    "where:\n",
    "\n",
    "global_norm = sqrt(sum([l2norm(t)**2 for t in t_list])) <br><br>\n",
    "If clip_norm > global_norm then the entries in t_list remain as they are, otherwise they're all shrunk by the global ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anneal the learning rate(튜닝) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizers accept both scalars and tensors as learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률에 step마다 decay를 가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate = tf.train.exponential_decay(init_lr,\n",
    " global_step,\n",
    " decay_steps,\n",
    " decay_rate,\n",
    " staircase=True) <br>\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applies exponential decay to the learning rate.\n",
    "\n",
    "decayed_learning_rate = learning_rate *\n",
    "                        decay_rate ^ (global_step / decay_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User dropout through tf.nn.dropout or DropoutWrapper for cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.nn.dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DropoutWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cell = tf.nn.rnn_cell.GRUCell(hidden_size)<br>\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    " output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
