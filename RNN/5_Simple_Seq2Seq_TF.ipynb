{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from konlpy.tag import Mecab; m = Mecab()\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "김진중님 코드 : https://github.com/golbin/TensorFlow-Tutorials/blob/master/06%20-%20RNN%2C%20ChatBot/03%20-%20Seq2Seq.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ONE_HOT = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드 및 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch = [batch_size,time_step, input_dimension] 으로 맞춰준다..! <br>\n",
    "후에 다시 인코더에 넣을 때는, [time_step,batch_size,input_dimension]으로 바꿔줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/dialogue_sample.txt', 'r', encoding='utf-8') as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "conv=[]\n",
    "for r in raw:\n",
    "    if r !='\\n':\n",
    "        conv.append(r)\n",
    "\n",
    "raw_X = [r.split('\\\\t')[0] for r in conv] # 리얼 인풋\n",
    "raw_y = [r.split('\\\\t')[1][:-1] for r in conv] # 리얼 아웃풋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(x,Vocab_size):\n",
    "    return np.identity(Vocab_size)[x:x+1]\n",
    "\n",
    "def vocab_encode(text, vocab):\n",
    "    \"\"\"\n",
    "    text가 Vocab(characters) 안에 있으면 index로 변환\n",
    "    \"\"\"\n",
    "    return [vocab.index(x) for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    \"\"\"\n",
    "    index를 다시 문자열로 디코딩\n",
    "    \"\"\"\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def posTag(sentences):\n",
    "    result = []\n",
    "    for sent in sentences:\n",
    "        temp = m.morphs(sent)\n",
    "        result.append(temp)\n",
    "    return result\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘 스케줄이 있는가?'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_X[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = posTag(raw_X)\n",
    "y = posTag(raw_y)\n",
    "tokenset = flatten(X+y)\n",
    "tokenset.extend(['<UNK>','<NUM>','<PAD>','<END>'])\n",
    "tokenset = list(set(tokenset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '스케줄', '이', '있', '는가', '?']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in X:\n",
    "    left = 10-len(x)\n",
    "    x.extend(['<PAD>']*left)\n",
    "    \n",
    "for y_ in y:\n",
    "    y_.append('<END>')\n",
    "    left = 10-len(y_)\n",
    "    y_.extend(['<PAD>']*left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['오늘', '스케줄', '이', '있', '는가', '?', '<PAD>', '<PAD>', '<PAD>', '<PAD>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = [vocab_encode(x,tokenset) for x in X]\n",
    "y_batch = [vocab_encode(y_,tokenset) for y_ in y]\n",
    "target_batch = [vocab_encode(y_,tokenset) for y_ in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[196, 224, 90, 15, 122, 232, 207, 207, 207, 207]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenset)\n",
    "batch_size = len(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if ONE_HOT:\n",
    "    for x in range(len(x_batch)):\n",
    "        for j in range(len(x_batch[x])):\n",
    "            x_batch[x][j] = one_hot(x_batch[x][j],vocab_size)\n",
    "\n",
    "    for y_ in range(len(y_batch)):\n",
    "        for j in range(len(y_batch[y_])):\n",
    "            y_batch[y_][j] = one_hot(y_batch[y_][j],vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = [np.array(x) for x in x_batch]\n",
    "y_batch = [np.array(y_) for y_ in y_batch]\n",
    "\n",
    "x_batch = np.squeeze(x_batch)\n",
    "y_batch = np.squeeze(y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10, 268)\n",
      "(100, 10, 268)\n"
     ]
    }
   ],
   "source": [
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 빌드 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch = [time_step,batch_size,input_dimension]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_classes = vocab_size # one-hot이니까..\n",
    "#n_input = 50 # embedding\n",
    "n_hidden = 128\n",
    "n_layers = 3\n",
    "time_step = 10\n",
    "n_step = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholder, Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########\n",
    "# 신경망 모델 구성\n",
    "######\n",
    "# Seq2Seq 모델은 인코더의 입력과 디코더의 입력의 형식이 같다.\n",
    "# [batch size, time steps, input size]\n",
    "\n",
    "#enc_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "#dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "# [batch size, time steps]\n",
    "\n",
    "\n",
    "if ONE_HOT:\n",
    "    enc_input = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
    "    dec_input = tf.placeholder(tf.float32, [None, None, vocab_size])\n",
    "    targets = tf.placeholder(tf.int64, [None, None])\n",
    "    \n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    \n",
    "    W = tf.Variable(tf.ones([n_hidden, n_classes]))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "    # tf.nn.dynamic_rnn 옵션에서 time_major 값을 True 로 설정\n",
    "    # [batch_size, n_steps, n_input] -> Tensor[n_steps, batch_size, n_input]\n",
    "    enc_input = tf.transpose(enc_input, [1, 0, 2])\n",
    "    dec_input = tf.transpose(dec_input, [1, 0, 2])\n",
    "    \n",
    "else:\n",
    "    x_input = tf.placeholder(tf.int32, [None, time_step])\n",
    "    y_input = tf.placeholder(tf.int32, [None, time_step])\n",
    "    targets = tf.placeholder(tf.int64, [None, None])\n",
    "    \n",
    "    embed_matrix = tf.Variable(tf.random_uniform([vocab_size, n_input], -1.0, 1.0),name='embed_matrix')\n",
    "\n",
    "    enc_input = tf.nn.embedding_lookup(embed_matrix, x_input, name='embed')                                                                    \n",
    "    dec_input = tf.nn.embedding_lookup(embed_matrix, y_input, name='embed')  \n",
    "\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    W = tf.Variable(tf.ones([n_hidden, n_classes]))\n",
    "    b = tf.Variable(tf.zeros([n_classes]))\n",
    "\n",
    "    # tf.nn.dynamic_rnn 옵션에서 time_major 값을 True 로 설정\n",
    "    # [batch_size, n_steps, n_input] -> Tensor[n_steps, batch_size, n_input]\n",
    "    enc_input = tf.transpose(enc_input, [1, 0, 2])\n",
    "    dec_input = tf.transpose(dec_input, [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('encode'):\n",
    "    enc_cell = tf.contrib.rnn.BasicRNNCell(n_hidden)\n",
    "    enc_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([enc_cell] * n_layers)\n",
    "    \n",
    "    outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input ,\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "# 디코더 셀을 구성한다.\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.contrib.rnn.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([dec_cell] * n_layers)\n",
    "\n",
    "    # Seq2Seq 모델은 인코더 셀의 최종 상태값을\n",
    "    # 디코더 셀의 초기 상태값으로 넣어주는 것이 핵심.\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=enc_states,\n",
    "                                            dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost, Op_f "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[batch size, time steps, hidden layers] <br>\n",
    "[batch size\\*time_steps, hidden layer] <br>\n",
    "[batch size * time steps, class numbers] <br>\n",
    "[batch size, time steps, class numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sparse_softmax_cross_entropy_with_logits 함수를 사용하기 위해\n",
    "# 각각의 텐서의 차원들을 다음과 같이 변형하여 계산한다.\n",
    "#    -> [batch size, time steps, hidden layers]\n",
    "time_steps = tf.shape(outputs)[1]\n",
    "#    -> [batch size * time steps, hidden layers]\n",
    "outputs_trans = tf.reshape(outputs, [-1, n_hidden])\n",
    "#    -> [batch size * time steps, class numbers]\n",
    "logits = tf.matmul(outputs_trans, W) + b\n",
    "#    -> [batch size, time steps, class numbers]\n",
    "logits = tf.reshape(logits, [-1, time_steps, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels= targets))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost,global_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2000 , cost = 0.6143342852592468\n",
      "최적화 완료!\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('ckp/05/checkpoint'))\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print('restore')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "for epoch in range(n_step):\n",
    "    _, loss = sess.run([train_op, cost],\n",
    "                        feed_dict={enc_input: x_batch, #one-hot enc_input:\n",
    "                                   dec_input: y_batch, #one-hot dec_input:\n",
    "                                   targets: target_batch})\n",
    "    sys.stdout.write(\"\\rEpoch: {} , cost = {}\".format(epoch+1,loss))\n",
    "    sys.stdout.flush()\n",
    "global_step_for_save = sess.run(global_step)\n",
    "saver.save(sess, 'ckp/05/simple_seq.model', global_step_for_save)\n",
    "print('\\n최적화 완료!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "ckpt = tf.train.get_checkpoint_state(os.path.dirname('ckp/05/checkpoint'))\n",
    "if ckpt and ckpt.model_checkpoint_path:\n",
    "    print('Ready!')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print('Please save session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_test(index):\n",
    "    prediction = tf.argmax(logits, 2)\n",
    "    prediction_check = tf.equal(prediction, targets)\n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction_check, tf.float32))\n",
    "\n",
    "    real, predict, accuracy_val = sess.run([targets, prediction, accuracy],\n",
    "                                           feed_dict={enc_input: x_batch[index:index+1], #one-hot enc_input:\n",
    "                                                      dec_input: y_batch[index:index+1], #one-hot dec_input:\n",
    "                                                      targets: target_batch[index:index+1]})\n",
    "    \n",
    "    \n",
    "    print_pred = []\n",
    "    for p in predict[0]:\n",
    "        dec = tokenset[p]\n",
    "        if dec == '<END>':\n",
    "            break\n",
    "        print_pred.append(dec)\n",
    "    \n",
    "    print(\"\\n=== 예측 결과 ===\")\n",
    "    # print('입력값:', X[index])\n",
    "    #print('실제값:', [[tokenset[j] for j in dec] for dec in real])\n",
    "    #print('예측값:', [[tokenset[i] for i in dec] for dec in predict])\n",
    "    #print('정확도:', accuracy_val)\n",
    "    \n",
    "    print('입력값:',raw_X[index])\n",
    "    print('실제값:',raw_y[index])\n",
    "    \n",
    "    \n",
    "    print('예측값:',' '.join(print_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 예측 결과 ===\n",
      "입력값: 오호 괜찮네\n",
      "실제값: 감사합니다\n",
      "예측값: 얘 에서 끝내 야 야 고마워 뭔데 고마워 뭔데 에서\n"
     ]
    }
   ],
   "source": [
    "randomNum = random.choice(range(batch_size))\n",
    "prediction_test(randomNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "패딩으로 인풋과 아웃풋의 길이를 고정했음.!! loss에 padding도 포함되기 때문에 정확한 loss가 아니다?!  <br>\n",
    "TF에서 제공하는 PAD도 사용해보기 <br>\n",
    "또한 인풋의 차원을 이랬다가저랬다가 너무나 복잡한것.... 좀 더 쉬운 방법 없나? <br>\n",
    "reshape와 transpose를 확실하게 정리하자\n",
    "Tensorarray ?<br>\n",
    "인풋을 임베딩 매트릭스로 lookup 하고 싶은데..? (이것도 jointly training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "아니 왜 placeholder에 다른 차원을 흘려보낸다음에 걔를 reshape하면 수렴을 안하는거지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
