{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level rnn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = './data/arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    \"\"\"\n",
    "    text가 Vocab(characters) 안에 있으면 index로 변환\n",
    "    \"\"\"\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    \"\"\"\n",
    "    index를 다시 문자열로 디코딩\n",
    "    \"\"\"\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS/2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "\n",
    "        for start in range(0, len(text) - window, int(overlap)):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # 초기 state를 default로 다 0인 벡터로 잡기?\n",
    "    \n",
    "    # 시퀀스의 실제 길이를 저장한다\n",
    "    # 모든 시퀀스는 NUM_STEPS만큼으로 padding 된다\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    \n",
    "    # dynamic_rnn을 사용하려면 실제 길이를 알려줘야 함(length)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    \n",
    "    return output, in_state, out_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.sign(x, name=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns an element-wise indication of the sign of a number.\n",
    "\n",
    "y = sign(x) = -1 if x < 0; 0 if x == 0; 1 if x > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.reduce_max(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the maximum of elements across dimensions of a tensor.\n",
    "\n",
    "Reduces input_tensor along the dimensions given in axis. Unless keep_dims is true, the rank of the tensor is reduced by 1 for each entry in axis. If keep_dims is true, the reduced dimensions are retained with length 1.\n",
    "\n",
    "If axis has no entries, all dimensions are reduced, and a tensor with a single element is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.random.randn(2,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = np.array([[1,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.reduce_max(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = np.array([[1,1,1],[1,1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.reduce_sum(input_tensor, axis=None, keep_dims=False, name=None, reduction_indices=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[2 2 2]\n",
      "[3 3]\n",
      "[[3]\n",
      " [3]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.reduce_sum(test)))\n",
    "    print(sess.run(tf.reduce_sum(test,0)))\n",
    "    print(sess.run(tf.reduce_sum(test,1)))\n",
    "    print(sess.run(tf.reduce_sum(test,1,keep_dims=True)))\n",
    "    print(sess.run(tf.reduce_sum(test,[0,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None, dtype=None, parallel_iterations=None, swap_memory=False, time_major=False, scope=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uses a tf.While loop to dynamically construct the graph when it is executed. Graph creation is faster and you can feed batches of variable size. (다 다른 문장 길이를 동적으로 생성. 오히려 더 빠르다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1],labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('ckp/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_intference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'ckp/arvix/char-rnn', iteration)\n",
    "            iteration += 1\n",
    "\n",
    "def online_intference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9483.3203125. Time 10.39647650718689\n",
      "T7oe   e                                                                                                                                                e          e    e   e   e   e   e    e   e   e   e   e   e   e   e   e   e   e   e   e   e  e   e  e  e   e   e   e  e  e   e  e   e  e  e  e  e  e  \n",
      "Iter 79. \n",
      "    Loss 8448.4296875. Time 11.026517391204834\n",
      "T3y the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 119. \n",
      "    Loss 7811.4736328125. Time 9.393127679824829\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 159. \n",
      "    Loss 7289.451171875. Time 10.370106220245361\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 199. \n",
      "    Loss 6811.9033203125. Time 10.6536865234375\n",
      "The the sent and of the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent and the sent\n",
      "Iter 239. \n",
      "    Loss 6529.875. Time 9.429558277130127\n",
      "The the sed and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 279. \n",
      "    Loss 6266.6015625. Time 8.62903356552124\n",
      "The senconel networks the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the \n",
      "Iter 319. \n",
      "    Loss 6455.482421875. Time 8.639407634735107\n",
      "The sure and in the semple and the are the searning are the are the searning are the are the are the searning are the are the are the stodes and reconter and reconter and reconter and reconter and reconter and reconter and reconter and reconter and in the semple and the are the are the stodes and rec\n",
      "Iter 359. \n",
      "    Loss 5730.3681640625. Time 8.633947134017944\n",
      "The semples and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architection and the architec\n",
      "Iter 399. \n",
      "    Loss 5543.40673828125. Time 8.63356900215149\n",
      "The stachised for the results and the are the results and the are the results and the are the results and the are the results and the are the results and the are the results and the are the results and the networks the results and the are the results and the are the results and the are the results an\n",
      "Iter 439. \n",
      "    Loss 5321.8515625. Time 9.000402450561523\n",
      "The the componed to the converution of the proposed to the converution of the proposed to the converution of the proposed to the converution of the proposed to the converution of the proposed to the converution of the proposed to the converution of the proposed to the converution of the convex to the\n",
      "Iter 479. \n",
      "    Loss 4654.48193359375. Time 8.609676361083984\n",
      "The such as our a proposed in the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the such as the suc\n",
      "Iter 519. \n",
      "    Loss 4953.14794921875. Time 8.634225606918335\n",
      "The convexion the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of the recent the componet of t\n",
      "Iter 559. \n",
      "    Loss 5485.4404296875. Time 8.68077278137207\n",
      "The such as the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the sume algorithm in the\n",
      "Iter 599. \n",
      "    Loss 4741.89111328125. Time 8.63053035736084\n",
      "The the results and state-of-the-art networks. The standard deep neural networks the networks the networks the networks (DNNs) and standard methods that the networks (DNNs) and standard methods that the networks (DNNs) and standard methods that the networks (DNNs) and standard methods that the networ\n",
      "Iter 639. \n",
      "    Loss 4167.17578125. Time 8.638767719268799\n",
      "The convergence of the and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and an\n",
      "Iter 679. \n",
      "    Loss 4356.5888671875. Time 8.63471007347107\n",
      "The recurrent neural networks (DNNs) and state-of-the-art recurrent neural networks (DNNs) and state-of-the-art recurrent neural networks (DNNs) and state-of-the-art recurrent neural networks (DNNs) and state-of-the-art recurrent neural networks (DNNs) and state-of-the-art recurrent neural networks (\n",
      "Iter 719. \n",
      "    Loss 3745.899658203125. Time 9.024294376373291\n",
      "The reculters the such as the success to the network processing and and computational neural networks to the success and computational neural networks to the network processing and and computational neural networks to the network processing and and computational neural networks to the network process\n",
      "Iter 759. \n",
      "    Loss 4158.87109375. Time 8.670763492584229\n",
      "The results on a simple experiments of a simple entome sereation and the different descent that the convex of the and the computed for the layer of a new formentional parameters of a computation of a new formentional parameters of a computation of a new formentional parameters of a computation of a n\n",
      "Iter 799. \n",
      "    Loss 4203.02880859375. Time 8.650349140167236\n",
      "The seter transformations are architectures the such as inversitional learning algorithms and state-of-the-art propose a deep learning algorithms in the seterations are architectures the such as improvement of the sementional state-of-the-art recurrent networks training state-of-the-art in training d\n",
      "Iter 839. \n",
      "    Loss 3599.376953125. Time 8.64228105545044\n",
      "The results and in a speed us a tempropagent and the method in the conventional networks and computational networks and computation of the and deep learning architectures and complexing a convexitional networks and computational networks and computation of the and deep learning architectures and comp\n",
      "Iter 879. \n",
      "    Loss 3466.73828125. Time 8.628142833709717\n",
      "The reculter stati-gradient descent that the computation to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to the distribution to t\n",
      "Iter 919. \n",
      "    Loss 3599.046875. Time 8.628615140914917\n",
      "The recently proposed neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural networks (DNNs) a deep neural ne\n",
      "Iter 959. \n",
      "    Loss 3727.505859375. Time 8.657898426055908\n",
      "The convolutional neural networks to stochastic gradient descent that convergence the convolutional neural networks to learn the computation to the computation to the computation to the computation to the computation to the computation to the computation to the computation to the computation to the c\n",
      "Iter 999. \n",
      "    Loss 3376.99951171875. Time 8.655332803726196\n",
      "The experiments of the input and in a deep neural network (DNN) models and state-of-the-art mask on the input and in a deep neural network (DNN) models and in a small DNN accuracy of the input and in a deep neural network (DNN) models and state-of-the-art mask on the input and in a deep neural networ\n",
      "Iter 1039. \n",
      "    Loss 3700.35888671875. Time 8.623159646987915\n",
      "The reculred models a conticurary stati-order to the properties of the proposed models and a contical recognition to prediction of the main relations. We propose a neural network is a correct recognition to prects experiments and a correctly properties of the proposed pooling our networks. We propose\n",
      "Iter 1079. \n",
      "    Loss 3400.81689453125. Time 8.629451990127563\n",
      "The result descent in a speed-uper in a parameters of the approach to the designed to the convergence rates of the approach to the designed to the convergence rates of the approach to the designed to the convergence rates of the approach to the designed to the convergence rates of the approach to the\n",
      "Iter 1119. \n",
      "    Loss 3545.45068359375. Time 8.670842170715332\n",
      "The activation to a supervised linear approach to the context of the approach of the network deep learning algorithm in the recuart to train a seme-training deep learning algorithm is the training deep learning algorithms that the proposed pooling operations to the network deep learning algorithm in \n",
      "Iter 1159. \n",
      "    Loss 3055.1982421875. Time 8.631263732910156\n",
      "The recent resurts that representation and in not an neural networks that computation in the results of the local linear network training and computation in the results of the layers that computation in the results of the layers that computation in the results of the layers that computation in the re\n",
      "Iter 1199. \n",
      "    Loss 3747.96044921875. Time 8.637765645980835\n",
      "The results on the input dataset structured sparsity points in the training distribution of the input dataset structured successful approach for dimension to be used in a deep neural network that is strategy the training distribution of the input dataset structured supprrite the suggest that is strat\n",
      "Iter 1239. \n",
      "    Loss 3290.28955078125. Time 8.640503644943237\n",
      "The recently proposed methods to be a deep learning mathic in a simple model as can be used in a significantly interarized a group a temper a parametrization of a neural network in computational training distributions, a deep learning algorithms that based on a non-convex optimization processing and \n",
      "Iter 1279. \n",
      "    Loss 3342.34326171875. Time 8.6252121925354\n",
      "The convolutional networks are computational training the convolutional networks are computational training the convolutional networks are computational training the convolutional networks are computational training the convolutional networks are computational training the convolutional networks are \n",
      "Iter 1319. \n",
      "    Loss 3295.98193359375. Time 8.637487888336182\n",
      "The expers the experiments on the experiments on the interact of the network and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "Iter 1359. \n",
      "    Loss 3163.85205078125. Time 8.641682863235474\n",
      "The successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and show that the successes and sho\n",
      "Iter 1399. \n",
      "    Loss 3402.029541015625. Time 10.335410594940186\n",
      "The convex optimization problem in a distributed computational networks are accuracy of the network distributed dataset such as important for a theoretical and the accuracy achieve the complexity of a computational networks and discriminate designed to computer vision to a simple complexity of a meth\n",
      "Iter 1439. \n",
      "    Loss 3445.9140625. Time 10.084040641784668\n",
      "The approach is a supervised learning algorithm is approach is a deep learning algorithm is and transfer network significantly repaitive the space to a subjectnolly to adversaries and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and a\n",
      "Iter 1479. \n",
      "    Loss 3207.6962890625. Time 8.826296329498291\n",
      "The results and in a search for rectified linear notwork architecture of the layers to adversaries and standard successes and first sent-tespert to full scale in the field state-of-the-art performance on a deep neural network (DNN) models and in a feed models and in a feed models and in a feed models\n",
      "Iter 1519. \n",
      "    Loss 3198.843505859375. Time 8.71723985671997\n",
      "The resulting the activation for the exproximation of a clustering and the results on a convergence rates of the interach framework size, reconstruction error in parallelizetwork processing with state-of-the-art deep networks (RNNs) are sound state-of-the-art the existing algorithm first intoration p\n",
      "Iter 1559. \n",
      "    Loss 3141.306396484375. Time 8.779551029205322\n",
      "The success of the training deep neural networks (DNN) that can be implemented with a novel computational results chanlecouptions.   Ove fixld pooling operative models with a simple maxout network (DNN) models as the parameter varieug local systems. This algorithm is the training deep neural network \n",
      "Iter 1599. \n",
      "    Loss 3153.413330078125. Time 8.689115047454834\n",
      "The convolutional networks are able to address the network is an approach is a convexity interpretational applications. Intaititial structures sparsity as a single mapping for the output of the input as approaches for the output of the input and the approach is a maxput of the layers to adversarial s\n",
      "Iter 1639. \n",
      "    Loss 2902.488037109375. Time 8.724202871322632\n",
      "These important and standard approach stard recognition to adversarial samples reduced and many state-of-the-art deep neural networks (RNNs) are some strategress and then for the log-linear neural networks and in a stati-timently improve the proposed neural networks and in a stati-timently improve th\n",
      "Iter 1679. \n",
      "    Loss 2974.567138671875. Time 8.70022702217102\n",
      "The algorithm for training deep neural networks (DNNs) are show that the context of deep neural networks (DNNs) are show that the context of deep neural networks (DNNs) are show that the context of deep neural networks (DNNs) are show that the context of deep neural networks (DNNs) are show that the \n",
      "Iter 1719. \n",
      "    Loss 2803.6044921875. Time 8.766836643218994\n",
      "The convolutional networks to the network prediction of the network is experimental results on the network is experimental results on the network is experimental results on the network is experimental results on the network is experimental results on the network is experimental results on the network\n",
      "Iter 1759. \n",
      "    Loss 3145.052490234375. Time 8.71987009048462\n",
      "The altorithm in the context of deep neural networks (DNNs) and results over the model structure of comperitional compared to the design deep networks on MBN that can be used to predind the and the and maxout unit (CIF) principlecensing the and maxout networks (DBN) have been sely models the predicti\n",
      "Iter 1799. \n",
      "    Loss 2977.055419921875. Time 8.695265293121338\n",
      "The results on the training of the input datasets. In this paper, we propose a novel framework for the training of the input datasets. In this paper, we propose a novel framework for the training of the input datasets. In this paper, we propose a novel framework for the training of the input datasets\n",
      "Iter 1839. \n",
      "    Loss 2772.990234375. Time 10.488791704177856\n",
      "The convergence rates, and in a reaut features that are complexity of the network structure of the neural networks with a convergence rates of the neural network that are communication and the convex the convex into the common of complexity introduced by the neural networks with sequence accurate in \n",
      "Iter 1879. \n",
      "    Loss 2956.30419921875. Time 9.002197265625\n",
      "The algorithm is to the existing a general for the speedups the effectiveness and algorithm for training datasets and speech recognition system speciagien state-of-the-art the effectiveness and speed using a general for the speedups of the approximate structure of the approach to train a mathi- compu\n",
      "Iter 1919. \n",
      "    Loss 2859.161376953125. Time 8.69067931175232\n",
      "The map pooling results on the complexity of success and computational computation in the result is parallel staudary solviding the accuracy. We also show how architecture to a feature is trained with provide the algorithms to obtain a classification to exploit machine learning tesm to a fundtmentati\n",
      "Iter 1959. \n",
      "    Loss 2783.6328125. Time 8.71863842010498\n",
      "These the accord in the convex optimization problem in machines to learn for description werk or better porally the state-of-the-art neural networks are algorithms to as a simple and then is an important application of the structures to learn for deep networks of compression from the locality. We exp\n",
      "Iter 1999. \n",
      "    Loss 2801.20703125. Time 8.723762273788452\n",
      "The experiments and stacking a convergence of the recently introduced and show that the training datasets. Secondly, we propose a novel for the effectiveness of a network architecture of the successis and computational results can be are assical learning results on a non-convex optimization, and show\n",
      "Iter 2039. \n",
      "    Loss 2716.224609375. Time 8.719405889511108\n",
      "The components intermation successful the complexity of starding, the network precessing successful the complexity of starding, the network precessing successful the complexity of starding, the network precessing successful the complexity of starding, the network precessing successful the complexity \n",
      "Iter 2079. \n",
      "    Loss 2794.85205078125. Time 8.709365129470825\n",
      "The a different RFNs and the approaches showorks (DNN) to a stati-tires layers that data and then for deep networks on learn fields to develop that is standard RNNs results on MBN has leading the and of the information processing standard RNN and on the information proporents (DBN) have shown that th\n",
      "Iter 2119. \n",
      "    Loss 2750.408447265625. Time 10.546062707901001\n",
      "The experimental results on the factorization using gradient descent of decain networks, the proposed methods are show that features that each machine learning the sequence of sequence of stacks. The generators can be mathid work? What representations are and maxout networks (each state-of-the-art re\n",
      "Iter 2159. \n",
      "    Loss 2786.900390625. Time 8.97522521018982\n",
      "The composition to the deep learning architectures of the network parameters and the non-convex explainstwo depreyer a new precessing layer-wise pre-training and deep neural networks that a convex explains the model computer vision our previous during the conventional layer-wise pre-training and deep\n",
      "Iter 2199. \n",
      "    Loss 2798.787353515625. Time 8.681421279907227\n",
      "The approximate the emils of computational results contally prediction to better representation of a network architecture of the input on the full models have been speed model accuracy. Our empirical initial context of computational complex invioning for functions that is imporsing tasks such as a co\n",
      "Iter 2239. \n",
      "    Loss 3004.1953125. Time 8.681763648986816\n",
      "The results introduced by a single layer as and computation in the training data that the proposed model as the approaches in the training data that the proposed model as the approaches in the training data that the proposed model as the approaches in the training data that the proposed model as the \n",
      "Iter 2279. \n",
      "    Loss 2649.49609375. Time 8.655304431915283\n",
      "These improve existing the achieve a simple and test this method within the convergence to story size of our and the are reariint to be addressed with a convergence rates of the network assestens. We introduce a new representation of the architecture can be related to be achieve a new first to achiev\n",
      "Iter 2319. \n",
      "    Loss 2743.314453125. Time 9.073846101760864\n",
      "The success of deep neural networks (DNN) have design the systems. Our results suggests that the successive arguine training to train a single layer at each layer of the successive and show that such as the local learning tasks. The proposed machine learning tasks. The learning tasks structure of the\n",
      "Iter 2359. \n",
      "    Loss 2618.55224609375. Time 8.633618831634521\n",
      "The approach is computing encoded in the complex to the speedup of the network are not only different structure of the networks are algobit, the importance of layers in the resulting a general network architectures (DNN) with CIFAR-100 fixtly the approach is computing the reconstructions in the compo\n",
      "Iter 2399. \n",
      "    Loss 2597.50439453125. Time 8.63889217376709\n",
      "The approach is a contextent data problems of convexity of the network contrastive directly on s method to be achieved to a learning architectures can be selected deep neural network (DNN) based on techniques are gradient resulte from and state-of-the-art matrix factor than the convergence rates in a\n",
      "Iter 2439. \n",
      "    Loss 2805.39453125. Time 8.675922393798828\n",
      "The submet by computation in the encoder as a single layer of the seanced gene-alt function from speech recognition systems with ressed synchrons of the recently proposed model as the effectiveness of a clustering machine learning model as the effectiveness of a cluster in the training deep neural ne\n",
      "Iter 2479. \n",
      "    Loss 2661.7783203125. Time 8.652045965194702\n",
      "The network proposed network problems with no deprovely to decomposition from the network proposed networks. We explore the model computationally expensive for the output of model to a large margin in deep neural networks (DNNs) and then implementation are of the network proposed networks. More recen\n",
      "Iter 2519. \n",
      "    Loss 2357.945556640625. Time 8.680018663406372\n",
      "The algorithm is evaluate the distribution to be trained on late--prorine learning algorithm for compersience training (source) and the desired systems with littroblemility problems and maxouting the state-of-the-art methods to and state-of-the-art deep neural networks (DNN) GPUs. The speed points a \n",
      "Iter 2559. \n",
      "    Loss 2315.426513671875. Time 8.705588340759277\n",
      "The experimentar a single layer becoderalization of the pre-ining a carefer and first-order information for the effectiveness of the approaches that the proposed method using machine learning to adversarial selections of the pre-ining architectures can requires learned size in the proposed method usi\n",
      "Iter 2599. \n",
      "    Loss 2583.0703125. Time 8.662461280822754\n",
      "The a simple and test time come from the convolutional neural networks that each size of the network depth and relited that is stracture of the network is stracture of the network convergence rates of the network is using the complexity of the complexity of MBN as the composition in a deep network ar\n",
      "Iter 2639. \n",
      "    Loss 2468.75146484375. Time 8.66969108581543\n",
      "The activation to the of the preprited of the system specification arguration compared to the convergence of the same output of a deep neural network to predict the designed to previous speech recognition in the tasks on a contextuch on a larger notes and standard and training deep networks are able \n",
      "Iter 2679. \n",
      "    Loss 2483.138427734375. Time 8.681859493255615\n",
      "The approach for data to preservised pooling recognition systems with ligear training (as a new particular, but algorithm between layers can be used for training deep neural networks. The proposed convolutional neural networks. We show how arbitrary sequcal learning rates of the approach is several w\n",
      "Iter 2719. \n",
      "    Loss 2453.796875. Time 8.660035610198975\n",
      "The algorithms such as stacked RNNs based optimization to be trained on these speedup convergence and models with a final DNN. We introduce RFNs or more developed to parallelism. We demonstrate that is strategy these functions that is significantly improvements in computation for speech recognition t\n",
      "Iter 2759. \n",
      "    Loss 2330.47509765625. Time 8.633725643157959\n",
      "The experiments in the emes the proposed $L_p$ units of deep neural network (DNN) models high computational results on deep neural network (DNN) as the training and show that the proposed model as the embedding approximations.  on the unit of the resulting modeling and matrix accuracy of the recently\n",
      "Iter 2799. \n",
      "    Loss 2323.915283203125. Time 8.684499502182007\n",
      "The compositional an an a fundated by the method for the output serve the compositional from the input of learning rates, and in parameters. The proposed network architectures are allower understanded and and the composetion of more approach is computation into present the first inference the network\n",
      "Iter 2839. \n",
      "    Loss 2646.71923828125. Time 8.653528928756714\n",
      "The resulting visual optimal many large non-convex optimization problem in the context of performance on stochastic gradient descent (s. Sicres of the network is encoded in the context of performance on stochastic gradient descent (s. Sicres of the network is encoded in the context of performance on \n",
      "Iter 2879. \n",
      "    Loss 2440.96826171875. Time 8.654259443283081\n",
      "The accuracy of the system specifications of the acculations for training deep neural networks (DNNs) as provabinity of the system speech recognition, visual object recognition systems can learning tasks. The staneard benainingly propactions of the acculations for training deep neural networks (DNNs)\n",
      "Iter 2919. \n",
      "    Loss 2580.8515625. Time 8.677587270736694\n",
      "The accuracy in parameter training deep neural networks are rone in computation interent and deep neural networks that computational convolutional neural networks that computational results on the main result deep networks and are alge introduces a converion for gradient descent in many or convergenc\n",
      "Iter 2959. \n",
      "    Loss 2375.35302734375. Time 8.665061950683594\n",
      "The activation functions and the proposed detochieres a context of complex bits in the individual training and the proposed detochide the results in large scale in our experiments show that by an understood in a different distribution of any learning parametrization and computation structure of defin\n",
      "Iter 2999. \n",
      "    Loss 1973.0648193359375. Time 8.662500619888306\n",
      "The conventional layer-wise (RNN) to decomposes syntetsicht the effectiveness of our neural networks (DNNs) as promising local learning rate been convex optimization performance on particular, the pottermont algorithms to active each parameters. The also propose a novel framework for training deep ne\n",
      "Iter 3039. \n",
      "    Loss 2485.05859375. Time 8.644038438796997\n",
      "The activations which are compositional computation in the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of the composition of\n",
      "Iter 3079. \n",
      "    Loss 2523.67578125. Time 8.69130563735962\n",
      "The existing emergation matrix computationally expensive to addressed within this paper proposed methods, provide gradient problem constructed that the proposed models have been the model as the effectiveness of MBN on unsupervised learning rate and unit in the relevant performance of an existing the\n",
      "Iter 3119. \n",
      "    Loss 2286.781005859375. Time 8.708214282989502\n",
      "The approaches and achieve simple experiments such low-dimensional contribution and the coordinates the layers and the amount of computation performance on a large DNN activating speech recognition in the tasks that are able to complicated to the input spack with understandaturepoltiblly test data. W\n",
      "Iter 3159. \n",
      "    Loss 2524.47900390625. Time 8.688695192337036\n",
      "The serias in a single factorization and strategy to courding a constrained problem and rates in a parameter algorithms to as a simple strategy where at task is sted essign. However, we present an optimal formoles, but cluster to an extraction of relevant for the loss function. This results suggests \n",
      "Iter 3199. \n",
      "    Loss 2476.64208984375. Time 8.711848258972168\n",
      "The subject in parameters that can be mating the learning tasks. The propagated of the learning tasks. The propagated of the learning tasks. The propagated of the learning tasks. The propagated of the learning tasks. The propagated of the learning tasks. The propagated of the learning tasks. The prop\n",
      "Iter 3239. \n",
      "    Loss 2412.353515625. Time 8.870811223983765\n",
      "The accurate between training deep neural networks to model that computation in the compositionally interpretation of the method to be to interpretation of the method to be to interpretation of the method to be to interpretation of the method to be to interpretation of the method to be to interpretat\n",
      "Iter 3279. \n",
      "    Loss 2384.593994140625. Time 8.7126305103302\n",
      "The activation with large number of predictions (DBM) and the construint descedurization to the high previous however experiments devently neural networks (DNN) showese models have been the model computational over other random GaMM) of the $L_p$ unit on the model inte proposed by Valdor. The over a \n",
      "Iter 3319. \n",
      "    Loss 2438.67578125. Time 8.719085931777954\n",
      "The experimental result has been selected stochastic frames. However, for the output segreat, such as stochastic gradient descent (SGD), it improved by combining feed-forward neural networks (DNNs) as pronercher state-of-the-art neural networks. The proposed $L_p$ unit on the recently proposed deep l\n",
      "Iter 3359. \n",
      "    Loss 2318.8037109375. Time 8.796618700027466\n",
      "The compositional can be a deep neural network architectures are general network architectures are general networks are accurase in computes with respect to a deep network architectures are general networks are accurase in computes with respect to a deep network architectures are general networks are\n",
      "Iter 3399. \n",
      "    Loss 2226.0439453125. Time 8.701611280441284\n",
      "The existing empirical provided for training on a larger learning and transparent matrix scaling the computationally expensive is the amount of machines conventional datasets (MAC novelized which each layer of the network by a difficulters and maint is the model computationally expensive to are and s\n",
      "Iter 3439. \n",
      "    Loss 2249.978515625. Time 8.867455959320068\n",
      "The expense of deep neural networks to the networks. The proposed Recreas exploit this paper, we propose a new fram wish latrers and them to complex to present novel LSTM by a shortcombeng the context order to learn for large-scale large neural network (DNN) to present a variety of deep neural networ\n",
      "Iter 3479. \n",
      "    Loss 2249.000244140625. Time 8.726462364196777\n",
      "The results in the compositional simple number of prediction for successful statial results of a cluster to altertation of the network depth and storiat cluster to address toparallelize for convergence rates of the indiviruation extensition the defense tens of neural networks in a distribution, it is\n",
      "Iter 3519. \n",
      "    Loss 2313.67626953125. Time 8.730987787246704\n",
      "The experiments of the approach computationally experiments show that the propased models can be used to train network to prodeden neural network to produce shart bounds from the layers that training large that the success results or graphsy successfully training compressive MBN used is as network si\n",
      "Iter 3559. \n",
      "    Loss 2308.201171875. Time 8.825855016708374\n",
      "The accurate cancupareng empirical size application to the not-siplly many specific dutactive gradient descent into the network processing and do show that the proposed deep learning algorithms have abshicely used for a variatoo-specified performance on a banddensimenters. The proposed model structur\n",
      "Iter 3599. \n",
      "    Loss 2223.783203125. Time 9.221848249435425\n",
      "The algorithm for convex to be fully reduce the sode speedup of a not diser and receatly achieved comparidation error in a different RPU which geves representation on the Wall Street Journal frameworks have open their benchmark machine learning architectures by a based hnal results in large number of\n",
      "Iter 3639. \n",
      "    Loss 2203.19580078125. Time 9.22062635421753\n",
      "The experiments of a method using stochastic gradient descent to represent the effective between training deep neural networks (DNN) to present a simple DNN acculations for capture that a simple DNN acculations for generalize to the system supervised learning and can be used to train network (DNN) ar\n",
      "Iter 3679. \n",
      "    Loss 2448.82958984375. Time 8.726326704025269\n",
      "The convolutional neural networks are accuracy is an approaches. We apply the proposed deep network architectures of a deep network architectures by parameters. The proposed methods of complex and deep learning algorithms is a cluster in a deep neural network architectures by a deep neural network ar\n",
      "Iter 3719. \n",
      "    Loss 2137.294189453125. Time 8.723373174667358\n",
      "The experimentally efficient methods of a clase bo training only and show that but also speed-oundrension that can be selved bootstrap models have evon a general many state-of-the-art open function and many specific defication for each layers; (i). Promutically, we propose a novel algorithms consiste\n",
      "Iter 3759. \n",
      "    Loss 2375.826171875. Time 8.80851435661316\n",
      "The exploit the system suggest to the stacking a partations to train machine learning results. We also find that tasks using stochastic gradient descent to representations effectiveness of a method for training DNN transpared feed for the system supports conventional layer-wise approximation of CIFAR\n",
      "Iter 3799. \n",
      "    Loss 2259.0009765625. Time 9.115796327590942\n",
      "The results in training only of the composed or images, spectrat variations. We propose a novel algorithms to as a single models in semention of stochastic miti-newernots of the art in a single models in the convergence rates, and the composed of computational complexity of stic structured sparser. W\n",
      "Iter 3839. \n",
      "    Loss 2202.41357421875. Time 8.6936674118042\n",
      "The existing empirical lovel one of the DNNs to achieve this paper we present a novel for the supervised learner. Tor objective to the input and on the maximum postibel of a parameter convergence gates, and the convergence of the activation function that is speed using and compression techniques and \n",
      "Iter 3879. \n",
      "    Loss 2203.28564453125. Time 8.961297273635864\n",
      "The expense of the system providing accurate capacity and computation of the network internal space. The inferemative us to the setting for autoencoder training and analyze their behavior of the input data supsoitasically learned for learning rules and show that the proposed by clain a weight matrix \n",
      "Iter 3919. \n",
      "    Loss 2244.604248046875. Time 8.674746990203857\n",
      "The resulting results show that both serias in a diverse set of existing results on a diverse set of existing results on a diverse set of experimental resulting in an auxiliary coordinate for the individual network architecture may note function of investimity of a clase function in the data from the\n",
      "Iter 3959. \n",
      "    Loss 2228.70703125. Time 8.658609628677368\n",
      "The existing approach of the recently introduced as a single machine model is task patable recurrent and the convex optimization to be a single layer acces on to train a deep network to produce spectrogram a simple model for the state-of-the-art mixture componed which data at regularizing the achieve\n",
      "Iter 3999. \n",
      "    Loss 2277.279541015625. Time 8.688380718231201\n",
      "The resulting architectures are pre-training algorithms to adversarial samples by a method to a be-paranter, algorithms to as a parameterization and achieves the method for training of neural networks to model as the tep on the method for provides depth for deep learning algorithms is computational n\n",
      "Iter 4039. \n",
      "    Loss 2034.02978515625. Time 8.713239669799805\n",
      "The successive approaches for training neural networks (DNN) have been the model structure of the non-point Into information such as the local variation based on tesprensimization and structure of a normalizing parallelism. We describe a group size and is to a stabili-nated fectiferticled recently in\n",
      "Iter 4079. \n",
      "    Loss 2322.5537109375. Time 8.847667694091797\n",
      "The existing results set of technonsultic works. with the inference is structured sparsi-grant domain adaptation, we propose a novel L-Trist of a method for the layers. Fist $P$ (MBFGis). The log time. We show that the training large network (DNN) to present a tephods of the sequence training, and di\n",
      "Iter 4119. \n",
      "    Loss 2381.413330078125. Time 8.70674443244934\n",
      "These in a diverse set of difficulties in the network parameters success and inte a smallow prople, batch normalization, where stochastic model their mechanism to accuracy. We also demonstrate has a pre-indaits and state of the art network. The approach is competitive with it is possible training. to\n",
      "Iter 4159. \n",
      "    Loss 2084.408203125. Time 8.773736476898193\n",
      "These of large-scale deep neural networks. We provide the class in the deep neural networks and maxout units matiging recurrent neural networks and maxout units matiging recurrent neural networks and maxout units matiging recurrent neural networks and maxout units matiging recurrent neural networks a\n",
      "Iter 4199. \n",
      "    Loss 1965.25927734375. Time 8.941810131072998\n",
      "The expense the existing equivity the stochastic gradient descent algorithm firthtwa promoding exploding speech recognition (e.g Krizative Nott getelige sigh size perform accuracy roccast up to be introduce a new learn relative to a search for a good generalization gradient processing accurately pate\n",
      "Iter 4239. \n",
      "    Loss 2132.799072265625. Time 8.731839656829834\n",
      "The resulting results show that our algorithm for convex ERMs is standard ConvNets + Marueading a novel fast, non-convex optimization of convex optimization and the aby popenties that is vect and fully connections (theeretional 4-5% relative in an autoencoder with deep neural networks with non-convex\n",
      "Iter 4279. \n",
      "    Loss 2087.397705078125. Time 8.685582160949707\n",
      "The existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of the existing approach of t\n",
      "Iter 4319. \n",
      "    Loss 2215.3544921875. Time 8.88120436668396\n",
      "The algorithms such as transformation of a between approaches that computation in the network internal neural networks that predictivencespenffuct activation function that implementations of the input data sets, we propose an approaches to the network internal neural networks that predictivencespenff\n",
      "Iter 4359. \n",
      "    Loss 2104.692626953125. Time 8.707775354385376\n",
      "The selection is tense from the loss for selecting between the model comperits reduces the advantage of deep neural networks are provides a new factor of binary constructed from the speed for large vory overcoder on sparse converges is to and are able to so convex ERMs is speakersily of the We probab\n",
      "Iter 4399. \n",
      "    Loss 1996.1400146484375. Time 8.754003524780273\n",
      "The existing results set of training datasets that is signing size. Elow ith as a set of relevant gates use the conventional labeling analyzing and the coordinates. In this paper, we show that training on a 300-hr two computational complexity of stacking maxout networks. The superior of large dataset\n",
      "Iter 4439. \n",
      "    Loss 2105.36474609375. Time 8.701587915420532\n",
      "The results computation performance on data that is ensimbles to better induces the application of a betcebrecent in computer vision tasks on different mappond that they training of non-convex optimization, and show that the proposed deep RNNs or gradients and computational networks to model computat\n",
      "Iter 4479. \n",
      "    Loss 2027.552978515625. Time 8.682084321975708\n",
      "The preserving and convergence rates in objective function and the proposed $L_p$ units related to train diefingence labeling and manimize the convolutional neural networks and many uping gradient descent (GD) based precentrobled in the factorization structure consisted constraints can be optimise su\n",
      "Iter 4519. \n",
      "    Loss 1984.110595703125. Time 8.644619226455688\n",
      "The existing a simple experiments using stochastic gradient descent hierarchical latent and show that much learn relative for large DNNs. We use the common language for decomposition, the proposed models of the system set complexity of this simple proie and faster region learning rates, and neurons e\n"
     ]
    }
   ],
   "source": [
    "vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "seq = tf.placeholder(tf.int32, [None, None])\n",
    "temp = tf.placeholder(tf.float32)\n",
    "loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
